{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a simple sentiment analysis model using GRU with the following components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer: Converts text to sequences of integers based on word frequency.\n",
    "pad_sequences: Ensures all sequences have the same length by padding or truncating.\n",
    "Embedding: Maps words to dense vector representations.\n",
    "GRU: Gated Recurrent Unit, a type of RNN for sequential data.\n",
    "Dense: Fully connected layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 12:43:28.862663: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-13 12:43:29.038817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-13 12:43:29.125562: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-13 12:43:29.152462: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-13 12:43:29.304618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 12:43:31.834399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences: A mix of positive and negative reviews.\n",
    "labels: Corresponding sentiment labels (1 for positive, 0 for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love this product\",\n",
    "    \"This is the worst experience\",\n",
    "    \"Absolutely amazing!\",\n",
    "    \"Not good at all\",\n",
    "    \"I'm very happy with this\",\n",
    "    \"Terrible service\",\n",
    "    \"Excellent quality\",\n",
    "    \"Awful, never buying again\",\n",
    "    \"Best purchase ever\",\n",
    "    \"Really bad, very disappointed\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer:\n",
    "\n",
    "    Assigns unique integers to each word.\n",
    "    Example: \"I love this product\" → [1, 2, 3, 4].\n",
    "\n",
    "pad_sequences:\n",
    "\n",
    "    Pads/truncates sequences to the same length (max_length).\n",
    "    Example: [1, 2, 3] → [1, 2, 3, 0] (padding with zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_length = max(len(s) for s in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layer:\n",
    "\n",
    "    Converts each word into a 16-dimensional dense vector.\n",
    "    Vocabulary size = len(tokenizer.word_index) + 1 (words + padding token).\n",
    "\n",
    "GRU Layer:\n",
    "\n",
    "    Processes sequences to capture temporal relationships.\n",
    "    16 units control the model's capacity.\n",
    "\n",
    "Dense Layer:\n",
    "\n",
    "    Outputs a single value between 0 and 1 (sigmoid activation).\n",
    "    Represents the probability of the review being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=16, input_length=max_length),\n",
    "    GRU(16),  \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Loss Function: binary_crossentropy for binary classification tasks.\n",
    "    Optimizer: adam for adaptive learning.\n",
    "    Training: Runs for 10 epochs with batch size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4667 - loss: 0.6935\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5000 - loss: 0.6900 \n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7014 - loss: 0.6837 \n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6736 - loss: 0.6799 \n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3806 - loss: 0.6876     \n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.6766 \n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6732 \n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.6651 \n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9667 - loss: 0.6580 \n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9458 - loss: 0.6516 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x72fc7413adb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=10, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and Padding Test Sentences:\n",
    "\n",
    "    Converts new reviews to sequences.\n",
    "    Pads to match the training input size.\n",
    "\n",
    "Making Predictions:\n",
    "\n",
    "    Outputs a probability for each review being positive.\n",
    "    Converts probabilities to sentiments: >0.5 → \"Positive\", <=0.5 → \"Negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"I hate this product\",\n",
    "    \"The service was excellent\",\n",
    "    \"Not worth the money\",\n",
    "    \"Absolutely fantastic experience!\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(test_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I hate this product\n",
      "Predicted Sentiment: Positive\n",
      "Review: The service was excellent\n",
      "Predicted Sentiment: Positive\n",
      "Review: Not worth the money\n",
      "Predicted Sentiment: Positive\n",
      "Review: Absolutely fantastic experience!\n",
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(test_sentences):\n",
    "    sentiment = \"Positive\" if predictions[i][0] > 0.5 else \"Negative\"\n",
    "    print(f\"Review: {sentence}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more data: Improve accuracy by training on a larger dataset.\n",
    "Tuning Hyperparameters: Adjust GRU units, embedding_dim, batch_size, and epochs.\n",
    "Regularization: Add dropout to avoid overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
